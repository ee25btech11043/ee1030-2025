\let\negmedspace\undefined
\let\negthickspace\undefined
\documentclass[journal,12pt,onecolumn]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{./figs/}}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{txfonts}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{gensymb}
\usepackage{comment}
\usepackage{caption}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} 
\usepackage{listings}
\usepackage{gvv}                                        
%\def\inputGnumericTable{}                                 
\usepackage[latin1]{inputenc}     
\usepackage{xparse}
\usepackage{color}                                            
\usepackage{array}
\usepackage{longtable}                                       
\usepackage{calc}                                             
\usepackage{multirow}
\usepackage{multicol}
\usepackage{hhline}                                           
\usepackage{ifthen}                                           
\usepackage{lscape}
\usepackage{tabularx}
\usepackage{array}
\usepackage{float}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}

\begin{document}

\title{Rotation Matrix}
\author{ee25btech11056 - Suraj.N}
\maketitle
\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}

\begin{document}

\begin{center}
$\vec{y} &= \vec{P}\vec{x}$ for all $\vec{x}$ .
\end{center}

\begin{align}
  \norm{\vec{y}}&=\norm{\vec{x}}\\
  \norm{\vec{y}}^2&=\norm{\vec{x}}^2\\
  (\vec{P}\vec{x})^\top\vec{P}\vec{x} &= \vec{x}^\top\vec{x}\\
  \vec{x}^\top(\vec{P}^\top\vec{P} - \vec{I})\vec{x} &= \vec{0}
\end{align}

Let $\vec{A} = \vec{P}^\top\vec{P} - \vec{I}$\\

$\vec{A}$ is symmetric because  
\begin{align}
  \left(\vec{P}^\top \vec{P} - \vec{I}\right)^\top = \vec{P}^\top \vec{P} - \vec{I}.
\end{align}

\textbf{Using eigen-decomposition for symmetric $\vec{A}$}  

By the spectral theorem, any real symmetric matrix $\vec{A}$ can be diagonalized by an orthogonal matrix $\vec{Q}$:  
\begin{align}
\vec{Q}^\top \vec{A} \vec{Q} = \vec{D},
\end{align}
where $\vec{D}$ is a diagonal matrix with the eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_n$ of $\vec{A}$.\\

\textbf{Express $\vec{x}^\top \vec{A} \vec{x}$ in terms of eigenvalues}  

For any vector $\vec{x}$, define $\vec{y} = \vec{Q}^\top \vec{x}$. Then,  
\begin{align}
\vec{x}^\top \vec{A} \vec{x} &= \vec{x}^\top \vec{Q} \vec{D} \vec{Q}^\top \vec{x} \\
&= \vec{y}^\top \vec{D} \vec{y} \\
&= \sum_{i=1}^{n} \lambda_i y_i^2
\end{align}

\textbf{Given $\vec{x}^\top \vec{A} \vec{x} = 0$ for all $\vec{x}$}  

Since $\vec{x}^\top \vec{A} \vec{x} = 0$ for every $\vec{x}$, it follows that  
\begin{align}
\sum_{i=1}^n \lambda_i y_i^2 = 0 \quad \text{for all } \vec{y}.
\end{align}

\textbf{Show all eigenvalues must be zero}  

This quadratic form is zero for all $\vec{y}$, meaning the sum of $\lambda_i y_i^2$ is always zero regardless of $\vec{y}$.  
Select vectors $\vec{y}$ which are zero everywhere except in the $i$-th coordinate, say $\vec{y} = \vec{e}_i$ (the $i$-th standard basis vector):  
\begin{align}
\sum_{i=1}^n \lambda_i y_i^2 = \lambda_i \cdot 1^2 = \lambda_i = 0.
\end{align}

Since this holds for each $i$, we conclude  
\begin{align}
\lambda_i = 0 \quad \text{for } i = 1,2,\dots,n.
\end{align}

\textbf{Conclusion -- $\vec{A}$ is the zero matrix}  

Because all eigenvalues of the symmetric matrix $\vec{A}$ are zero, it follows that  
\begin{align}
  \vec{D} = \vec{0} \implies \vec{A} = \vec{Q} \vec{D} \vec{Q}^\top = \vec{0}.
\end{align}

Therefore,  
\begin{align}
  \vec{P}^\top \vec{P} - \vec{I} = 0 \implies \vec{P}^\top \vec{P} = \vec{I}.
\end{align}

\textbf{Therefore the rotation matrix is orthogonal}\\

Deriving the Rotational matrix in 2D .

A rotation in $\mathbb{R}^n$ can be generated by exponentiating a skew-symmetric matrix $\vec{G}$:
\begin{align}
\vec{P}(\theta) = e^{\theta \vec{G}},
\end{align}
where
\begin{align}
\vec{G}^\top = -\vec{G},
\end{align}
and $\theta$ is the rotation angle.

In 2D, the fundamental skew-symmetric matrix is
\begin{align}
\vec{G} = \myvec{0 & -1 \\ 1 & 0}.
\end{align}

Note that $\vec{G}$ corresponds to a $90^\circ$ rotation in the plane.

Using the Taylor series expansion of the matrix exponential,
\begin{align}
e^{\theta \vec{G}} = \vec{I} + \theta \vec{G} + \frac{\theta^2}{2!}\vec{G}^2 + \frac{\theta^3}{3!}\vec{G}^3 + \cdots
\end{align}

Because
\begin{align}
\vec{G}^2 = -\vec{I},
\end{align}
the powers of $\vec{G}$ cycle as
\begin{align}
\vec{G}^0 &= \vec{I}, &
\vec{G}^1 &= \vec{G}, &
\vec{G}^2 &= -\vec{I}, &
\vec{G}^3 &= -\vec{G}, &
\vec{G}^4 &= \vec{I}, \; \text{and so on}.
\end{align}

This lets us rewrite the series as
\begin{align}
e^{\theta \vec{G}} = \vec{I}\left(1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \cdots \right)
+ \vec{G}\left(\theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \cdots \right).
\end{align}

Recognizing the Taylor series for $\cos \theta$ and $\sin \theta$, we obtain
\begin{align}
e^{\theta \vec{G}} = \vec{I}\cos \theta + \vec{G}\sin \theta.
\end{align}

Substituting back $\vec{G}$ and $\vec{I}$,
\begin{align}
\vec{P}(\theta) 
&= \cos\theta \myvec{1 & 0 \\ 0 & 1} 
+ \sin\theta \myvec{0 & -1 \\ 1 & 0} \\
&= \myvec{\cos \theta & -\sin \theta \\ \sin \theta & \cos \theta}.
\end{align}

\pagebreak 

Doing the same thing for 3D ,

Define the axis vector and skew-symmetric generator

Let $\vec{u} = \myvec{u_x \\ u_y \\ u_z}$ be a unit vector (axis of rotation) such that
\begin{align}
\|\vec{u}\| = 1.
\end{align}

Define the skew-symmetric matrix $\vec{G}$:
\begin{align}
\vec{G} = 
\myvec{
0 & -u_z & u_y \\
u_z & 0 & -u_x \\
- u_y & u_x & 0
}.
\end{align}

\textbf{Rotation matrix via exponential}

A rotation by angle $\theta$ about $\vec{u}$ is given by
\begin{align}
\vec{P}(\theta) = e^{\theta \vec{G}} = \sum_{n=0}^{\infty} \frac{(\theta \vec{G})^n}{n!}.
\end{align}

Powers of $\vec{G}$

It can be shown that
\begin{align}
\vec{G}^2 &= \vec{u}\vec{u}^\top - \vec{I}, \\
\vec{G}^3 &= \vec{G}(\vec{u}\vec{u}^\top - \vec{I}) = -\vec{G}.
\end{align}

\textbf{Series expansion}

Expanding the exponential,
\begin{align}
e^{\theta \vec{G}} = \vec{I} + \theta \vec{G} + \frac{\theta^2}{2!}\vec{G}^2 + \frac{\theta^3}{3!}\vec{G}^3 + \cdots
\end{align}

Grouping even and odd powers
\begin{align}
e^{\theta \vec{G}} 
= \vec{I} 
+ \left(\theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \cdots \right)\vec{G} 
+ \left(\frac{\theta^2}{2!} - \frac{\theta^4}{4!} + \cdots \right)\vec{G}^2.
\end{align}

Recognizing the Taylor series:
\begin{align}
\sin \theta &= \theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \cdots, \\
1 - \cos \theta &= \frac{\theta^2}{2!} - \frac{\theta^4}{4!} + \cdots,
\end{align}
we obtain
\begin{align}
e^{\theta \vec{G}} = \vec{I} + \sin \theta \, \vec{G} + (1 - \cos \theta)\vec{G}^2.
\end{align}

Substituting $\vec{G}^2$

Since $\vec{G}^2 = \vec{u}\vec{u}^\top - \vec{I}$,
\begin{align}
\vec{P}(\theta) 
= \cos \theta \, \vec{I} 
+ (1 - \cos \theta) \, \vec{u}\vec{u}^\top 
+ \sin \theta \, \vec{G}.
\end{align}

\textbf{Final Rotational Matrix}

Thus, the rotation matrix about axis $\vec{u}$ is
\begin{align}
\vec{P}(\theta) =
\myvec{
\cos \theta + u_x^2(1-\cos \theta) & u_x u_y(1-\cos \theta) - u_z \sin \theta & u_x u_z(1-\cos \theta) + u_y \sin \theta \\
u_y u_x(1-\cos \theta) + u_z \sin \theta & \cos \theta + u_y^2(1-\cos \theta) & u_y u_z(1-\cos \theta) - u_x \sin \theta \\
u_z u_x(1-\cos \theta) - u_y \sin \theta & u_z u_y(1-\cos \theta) + u_x \sin \theta & \cos \theta + u_z^2(1-\cos \theta)
}.
\end{align}


\end{document}
